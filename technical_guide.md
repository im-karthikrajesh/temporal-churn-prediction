# Technical Guide: Supporting Documentation for Churn Prediction Model

**Project Title:** Churn Prediction for FoodCorp

## Model Execution Instructions

The churn model is implemented end-to-end in a Databricks notebook using **DBR 15.4 LTS ML (Non-GPU)**. The code is modular and executes in a top-down manner without manual intervention. Follow these steps:

1. Load the notebook in Databricks under the specified ML runtime environment.
2. Ensure the provided Spark tables (`customers`, `receipts`, `receipt_lines`, `products`, `stores`) are accessible.
3. Execute the notebook from top to bottom. All intermediate feature engineering and modelling steps are handled sequentially.
4. Model outputs—probability forecasts, SHAP values, and all figures—will be generated as part of the pipeline.
5. No manual uploads or custom configurations are required.

## Data Cleaning Summary

### `receipt_lines`
- Descriptive statistics indicated negative quantities and values, and a placeholder value `9999999.0` in the `value` column.
- These anomalies were excluded by filtering negatives and removing the anomalous high value.
- A cleaned view, **`receipt_lines_clean`**, was created and used throughout the pipeline.

### `stores`
- Although there were no nulls, the longitude for **`store_code = 1`** was erroneous (`-1898193.0`), producing an unrealistic coordinate.
- This was corrected with the presumed location of **Birmingham** (longitude `-1.898193`).
- A cleaned view, **`stores_clean`**, was constructed with the actual coordinate and used downstream.

## Figure Mapping

| Figure | Method / Tool | Notes |
|---|---|---|
| **Figure 1: Temporal Window Diagram** | PowerPoint (manual illustration) | Report-only (not generated by code) |
| **Figure 2: Test Set ROC Curves (3 models)** | `matplotlib.pyplot` | ROC & AUC comparison across models |
| **Figure 3: Reliability Diagram (Raw vs Calibrated)** | `sklearn.calibration.calibration_curve` | Probability calibration |
| **Figure 4: Churn % by Predicted-Risk Decile** | `seaborn.barplot` | Decile / risk-bucket analysis |
| **Figure 5: Rolling Window Backtest** | `seaborn.lineplot` | Rolling-window backtest with Platt (sigmoid) calibration |
| **Figure 6: PDPs — Feature–Churn Relationships** | `sklearn.inspection.PartialDependenceDisplay` | Partial dependence for top 5 features |
| **Figure 7: SHAP — Global Feature Importance** | `seaborn.barplot` | TreeExplainer global importance |
| **Figure 8: SHAP — Beeswarm (Top 15 Features)** | `shap.summary_plot` | Local + global distribution |

## Dependencies and Runtime Notes

**Environment**
- Python **3.11.11**
- **Databricks Runtime 15.4 LTS ML (Non-GPU)**

**Key Libraries**
- Core: `math`, `datetime`, `tqdm`, `scipy.stats`
- Data handling: `pandas`, `numpy`
- Visualization: `matplotlib`, `seaborn`, `matplotlib.ticker.PercentFormatter`
- Spark: `pyspark.sql.functions` as `F`, `pyspark.sql.types.DecimalType`
- Scikit-Learn: `preprocessing`, `utils`, `calibration`, `metrics`, `inspection`
- Modeling: `sklearn.linear_model.LogisticRegression`, `sklearn.ensemble.RandomForestClassifier`, `xgboost.XGBClassifier`
- Interpretability: `shap`

**Additional Notes**
- SHAP values were computed on a **1,000-row** test sample for speed.
- PDPs were plotted for the **top 5 features** only.
- Calibration was tested using `calibration_curve()` with **10 uniform bins**.

---